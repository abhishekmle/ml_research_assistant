{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "    accelerate==1.5.2 \\\n",
    "    huggingface-hub==0.27.0 \\\n",
    "    safetensors==0.5.0 \\\n",
    "    sentence-transformers==3.3.1 \\\n",
    "    torch==2.6.0 \\\n",
    "    transformers==4.50.3 \\\n",
    "    langchain-community \\\n",
    "    langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:05:50.667801Z",
     "iopub.status.busy": "2025-05-22T12:05:50.667143Z",
     "iopub.status.idle": "2025-05-22T12:06:00.168048Z",
     "shell.execute_reply": "2025-05-22T12:06:00.167163Z",
     "shell.execute_reply.started": "2025-05-22T12:05:50.667772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-05-22 12:05:55.168116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-22 12:05:55.168174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-22 12:05:55.172737: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:06:03.325494Z",
     "iopub.status.busy": "2025-05-22T12:06:03.324836Z",
     "iopub.status.idle": "2025-05-22T12:06:03.399878Z",
     "shell.execute_reply": "2025-05-22T12:06:03.399013Z",
     "shell.execute_reply.started": "2025-05-22T12:06:03.325467Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:07:14.880549Z",
     "iopub.status.busy": "2025-05-22T12:07:14.879712Z",
     "iopub.status.idle": "2025-05-22T12:07:15.070536Z",
     "shell.execute_reply": "2025-05-22T12:07:15.069596Z",
     "shell.execute_reply.started": "2025-05-22T12:07:14.880513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20286, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "df = pd.read_parquet('ml_research_assistant/data/papers_with_abstract.parquet')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:07:16.636342Z",
     "iopub.status.busy": "2025-05-22T12:07:16.636008Z",
     "iopub.status.idle": "2025-05-22T12:07:16.654712Z",
     "shell.execute_reply": "2025-05-22T12:07:16.653743Z",
     "shell.execute_reply.started": "2025-05-22T12:07:16.636317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_year</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_author</th>\n",
       "      <th>paper_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2023/...</td>\n",
       "      <td>Modelling Cellular Perturbations with the Spar...</td>\n",
       "      <td>Michael Bereket, Theofanis Karaletsos</td>\n",
       "      <td>Generative models of observations under interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2023/...</td>\n",
       "      <td>Cross-Episodic Curriculum for Transformer Agents</td>\n",
       "      <td>Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby,...</td>\n",
       "      <td>We present a new algorithm, Cross-Episodic Cur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_year                                          paper_url  \\\n",
       "0        2023  https://papers.nips.cc/paper_files/paper/2023/...   \n",
       "1        2023  https://papers.nips.cc/paper_files/paper/2023/...   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Modelling Cellular Perturbations with the Spar...   \n",
       "1   Cross-Episodic Curriculum for Transformer Agents   \n",
       "\n",
       "                                        paper_author  \\\n",
       "0              Michael Bereket, Theofanis Karaletsos   \n",
       "1  Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby,...   \n",
       "\n",
       "                                      paper_abstract  \n",
       "0  Generative models of observations under interv...  \n",
       "1  We present a new algorithm, Cross-Episodic Cur...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:07:19.135457Z",
     "iopub.status.busy": "2025-05-22T12:07:19.134614Z",
     "iopub.status.idle": "2025-05-22T12:07:19.143786Z",
     "shell.execute_reply": "2025-05-22T12:07:19.143069Z",
     "shell.execute_reply.started": "2025-05-22T12:07:19.135425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_year</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_author</th>\n",
       "      <th>paper_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20284</th>\n",
       "      <td>1987</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/1987/...</td>\n",
       "      <td>Minkowski-r Back-Propagation: Learning in Conn...</td>\n",
       "      <td>Stephen Hanson, David Burr</td>\n",
       "      <td>Many connectionist learning models are impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20285</th>\n",
       "      <td>1987</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/1987/...</td>\n",
       "      <td>Analysis and Comparison of Different Learning ...</td>\n",
       "      <td>J. Bernasconi</td>\n",
       "      <td>We investigate the behavior of different learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper_year                                          paper_url  \\\n",
       "20284        1987  https://papers.nips.cc/paper_files/paper/1987/...   \n",
       "20285        1987  https://papers.nips.cc/paper_files/paper/1987/...   \n",
       "\n",
       "                                             paper_title  \\\n",
       "20284  Minkowski-r Back-Propagation: Learning in Conn...   \n",
       "20285  Analysis and Comparison of Different Learning ...   \n",
       "\n",
       "                     paper_author  \\\n",
       "20284  Stephen Hanson, David Burr   \n",
       "20285               J. Bernasconi   \n",
       "\n",
       "                                          paper_abstract  \n",
       "20284  Many connectionist learning models are impleme...  \n",
       "20285  We investigate the behavior of different learn...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:07:31.655311Z",
     "iopub.status.busy": "2025-05-22T12:07:31.654965Z",
     "iopub.status.idle": "2025-05-22T12:07:32.091496Z",
     "shell.execute_reply": "2025-05-22T12:07:32.090461Z",
     "shell.execute_reply.started": "2025-05-22T12:07:31.655286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "tensor([ 1.4846,  1.5773,  0.0996,  0.0618, -0.8350, -0.6634,  0.3206, -0.3603,\n",
      "         0.9237,  0.6285], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    x = torch.randn(10).cuda()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:07:51.011201Z",
     "iopub.status.busy": "2025-05-22T12:07:51.010822Z",
     "iopub.status.idle": "2025-05-22T12:07:52.066449Z",
     "shell.execute_reply": "2025-05-22T12:07:52.065341Z",
     "shell.execute_reply.started": "2025-05-22T12:07:51.011170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18071664\n",
      "drwxr-xr-x 3 nobody nogroup          0 Sep  3  2024 .\n",
      "drwxr-xr-x 3 root   root          4096 May 22 11:47 ..\n",
      "-rw-r--r-- 1 nobody nogroup       1661 Sep  3  2024 .gitattributes\n",
      "-rw-r--r-- 1 nobody nogroup      25789 Sep  3  2024 README.md\n",
      "-rw-r--r-- 1 nobody nogroup        857 Sep  3  2024 config.json\n",
      "-rw-r--r-- 1 nobody nogroup        173 Sep  3  2024 generation_config.json\n",
      "-rw-r--r-- 1 nobody nogroup 4903351912 Sep  3  2024 model-00001-of-00004.safetensors\n",
      "-rw-r--r-- 1 nobody nogroup 4947570872 Sep  3  2024 model-00002-of-00004.safetensors\n",
      "-rw-r--r-- 1 nobody nogroup 4962221464 Sep  3  2024 model-00003-of-00004.safetensors\n",
      "-rw-r--r-- 1 nobody nogroup 3670322200 Sep  3  2024 model-00004-of-00004.safetensors\n",
      "-rw-r--r-- 1 nobody nogroup      39072 Sep  3  2024 model.safetensors.index.json\n",
      "-rw-r--r-- 1 nobody nogroup        636 Sep  3  2024 special_tokens_map.json\n",
      "-rw-r--r-- 1 nobody nogroup   17525357 Sep  3  2024 tokenizer.json\n",
      "-rw-r--r-- 1 nobody nogroup    4241003 Sep  3  2024 tokenizer.model\n",
      "-rw-r--r-- 1 nobody nogroup      46996 Sep  3  2024 tokenizer_config.json\n",
      "drwxr-xr-x 2 nobody nogroup          0 Sep  3  2024 transformers\n"
     ]
    }
   ],
   "source": [
    "!ls -la /kaggle/input/gemma-2/transformers/gemma-2-9b-it/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:08:46.720093Z",
     "iopub.status.busy": "2025-05-22T12:08:46.719456Z",
     "iopub.status.idle": "2025-05-22T12:11:14.768436Z",
     "shell.execute_reply": "2025-05-22T12:11:14.767722Z",
     "shell.execute_reply.started": "2025-05-22T12:08:46.720058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469f89e1e486406faea0b13a3fb53665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load google gemma2-9b-it in 8-bit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=config_8bit,\n",
    "                                            device_map=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:11:25.595974Z",
     "iopub.status.busy": "2025-05-22T12:11:25.595103Z",
     "iopub.status.idle": "2025-05-22T12:11:25.609589Z",
     "shell.execute_reply": "2025-05-22T12:11:25.608921Z",
     "shell.execute_reply.started": "2025-05-22T12:11:25.595935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "hf_pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    ")\n",
    "local_llm = HuggingFacePipeline(pipeline=hf_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:15:58.545918Z",
     "iopub.status.busy": "2025-05-22T12:15:58.545565Z",
     "iopub.status.idle": "2025-05-22T12:15:59.278190Z",
     "shell.execute_reply": "2025-05-22T12:15:59.277402Z",
     "shell.execute_reply.started": "2025-05-22T12:15:58.545889Z"
    }
   },
   "outputs": [],
   "source": [
    "# we compare retrieval on the basis of a base model and a model that we finetuned\n",
    "encoder_original = SentenceTransformerEmbeddings(model_name='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:22:09.589063Z",
     "iopub.status.busy": "2025-05-22T12:22:09.588336Z",
     "iopub.status.idle": "2025-05-22T12:22:09.726384Z",
     "shell.execute_reply": "2025-05-22T12:22:09.725576Z",
     "shell.execute_reply.started": "2025-05-22T12:22:09.589031Z"
    }
   },
   "outputs": [],
   "source": [
    "# the finetuned model is in huggingface hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"YOUR_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-22T12:23:36.496342Z",
     "iopub.status.busy": "2025-05-22T12:23:36.495476Z",
     "iopub.status.idle": "2025-05-22T12:23:45.343763Z",
     "shell.execute_reply": "2025-05-22T12:23:45.342878Z",
     "shell.execute_reply.started": "2025-05-22T12:23:36.496312Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8601e07a4964455964bcc44ec612451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ab0824b84e441db5e9c40d72a9edbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23eaf02cdd0411b85c16cfd81f6f70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/42.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11615bdcc2d14f4089dab7e0154e054d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8968c050ee0f4e7e914fc2a93e5171d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/500 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6210547d9cc4b88a382294a27837eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65b9450703d4c80a84d37f284d73ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34b90887f9343af8425a402d6bd964a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7473a20c1549c28704b7494e1e0dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b115ad5b1db4c31814ed6ccf79f7283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe01a70e895483fba5c570103e89175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder_finetuned = SentenceTransformerEmbeddings(model_name='abhishekmle/distilbert-base-uncased-finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:23:54.959515Z",
     "iopub.status.busy": "2025-05-22T12:23:54.958708Z",
     "iopub.status.idle": "2025-05-22T12:23:54.965063Z",
     "shell.execute_reply": "2025-05-22T12:23:54.964076Z",
     "shell.execute_reply.started": "2025-05-22T12:23:54.959483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "), model_name='abhishekmle/distilbert-base-uncased-finetuned', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:26:47.230139Z",
     "iopub.status.busy": "2025-05-22T12:26:47.229081Z",
     "iopub.status.idle": "2025-05-22T12:26:47.303608Z",
     "shell.execute_reply": "2025-05-22T12:26:47.302855Z",
     "shell.execute_reply.started": "2025-05-22T12:26:47.230095Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:31:18.965700Z",
     "iopub.status.busy": "2025-05-22T12:31:18.965312Z",
     "iopub.status.idle": "2025-05-22T12:31:18.972191Z",
     "shell.execute_reply": "2025-05-22T12:31:18.971264Z",
     "shell.execute_reply.started": "2025-05-22T12:31:18.965671Z"
    }
   },
   "outputs": [],
   "source": [
    "def index_documents(self, docs: List[str]) -> None:\n",
    "    documents = [Document(page_content=d) for d in docs]\n",
    "    self.store = FAISS.from_documents(documents, self.embeddings)\n",
    "\n",
    "def index_documents_to_stores(\n",
    "    docs: List[str], \n",
    "    embeddings_dict: Dict[str, Embeddings]\n",
    ") -> Dict[str, FAISS]:\n",
    "    \"\"\"\n",
    "    Indexes the documents into separate FAISS stores, one for each embedding model.\n",
    "    \"\"\"\n",
    "    documents = [Document(page_content=d) for d in docs]\n",
    "    \n",
    "    stores = {}\n",
    "    for model_name, embedding in embeddings_dict.items():\n",
    "        stores[model_name] = FAISS.from_documents(documents, embedding)\n",
    "    \n",
    "    return stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:35:32.529151Z",
     "iopub.status.busy": "2025-05-22T12:35:32.528763Z",
     "iopub.status.idle": "2025-05-22T12:42:42.136836Z",
     "shell.execute_reply": "2025-05-22T12:42:42.136158Z",
     "shell.execute_reply.started": "2025-05-22T12:35:32.529118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20286\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = {\n",
    "    \"base_encoder\": encoder_original,\n",
    "    \"finetuned_encoder\": encoder_finetuned\n",
    "}\n",
    "\n",
    "docs = df['paper_abstract'].tolist()\n",
    "print(len(docs))\n",
    "\n",
    "stores = index_documents_to_stores(docs, embeddings_dict)\n",
    "# Now you can access stores[\"openai\"] and stores[\"hf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qualitative comparison between the un-tuned and the fine-tuned version of the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:51:27.996610Z",
     "iopub.status.busy": "2025-05-22T12:51:27.996229Z",
     "iopub.status.idle": "2025-05-22T12:51:28.000853Z",
     "shell.execute_reply": "2025-05-22T12:51:28.000032Z",
     "shell.execute_reply.started": "2025-05-22T12:51:27.996573Z"
    }
   },
   "source": [
    "#### 1st example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:46:44.836783Z",
     "iopub.status.busy": "2025-05-22T12:46:44.836397Z",
     "iopub.status.idle": "2025-05-22T12:46:44.859669Z",
     "shell.execute_reply": "2025-05-22T12:46:44.858934Z",
     "shell.execute_reply.started": "2025-05-22T12:46:44.836753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='a2bfbcb3-b7f9-4870-9494-44a3e695b108', metadata={}, page_content='This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show significant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods.'),\n",
       "  19.048574),\n",
       " (Document(id='235c3469-be19-4957-ac1f-d1ee6cdefb11', metadata={}, page_content='A short account is  given of various  investigations of neural  network  properties,  beginning  with  the  classic  work of McCulloch  & Pitts.  Early work on neurodynamics and statistical mechanics, analogies with  magnetic materials, fault tolerance via parallel distributed processing,  memory, learning,  and pattern recognition,  is described.'),\n",
       "  20.711102),\n",
       " (Document(id='df49ca06-e66e-4d11-80be-6ac4681bb549', metadata={}, page_content='Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.'),\n",
       "  21.003475)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['base_encoder'].similarity_search_with_score(query='Mathematical understanding of weight initialization methods in neural networks.', k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first two results of this un-tuned model are not relevant to the query, third result is somewhat relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:47:06.268572Z",
     "iopub.status.busy": "2025-05-22T12:47:06.267702Z",
     "iopub.status.idle": "2025-05-22T12:47:06.290946Z",
     "shell.execute_reply": "2025-05-22T12:47:06.290154Z",
     "shell.execute_reply.started": "2025-05-22T12:47:06.268540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='a47bfaa5-a116-4d16-8373-51408d8e34ee', metadata={}, page_content='Current deep neural networks(DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting function forms including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.'),\n",
       "  91.279144),\n",
       " (Document(id='0edc9981-a8e4-49b7-914d-51e751848a8b', metadata={}, page_content='It has  been observed  in  numerical simulations that a weight decay  can  im(cid:173) prove generalization in a feed-forward  neural network.  This paper explains  why.  It is  proven  that  a  weight  decay  has  two effects  in  a  linear  network.  First,  it  suppresses  any  irrelevant  components  of  the  weight  vector  by  choosing  the smallest  vector  that solves  the learning  problem.  Second,  if  the size  is chosen  right,  a weight  decay  can suppress some of the effects  of  static  noise  on  the  targets,  which  improves  generalization  quite  a  lot.  It  is  then  shown  how  to extend  these  results  to networks  with hidden  layers  and  non-linear  units.  Finally  the  theory  is  confirmed  by  some numerical  simulations using  the  data from  NetTalk.'),\n",
       "  95.263885),\n",
       " (Document(id='a50a4b5c-7d27-429c-adcd-0dbb51cd44db', metadata={}, page_content='Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights.Interactive version of this paper at https://weightagnostic.github.io/'),\n",
       "  96.17279)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['finetuned_encoder'].similarity_search_with_score(query='Mathematical understanding of weight initialization methods in neural networks.', k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all three results of the fine-tuned model are relevant to the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:53:07.812975Z",
     "iopub.status.busy": "2025-05-22T12:53:07.812279Z",
     "iopub.status.idle": "2025-05-22T12:53:07.836421Z",
     "shell.execute_reply": "2025-05-22T12:53:07.835652Z",
     "shell.execute_reply.started": "2025-05-22T12:53:07.812946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='4d131d8c-36a0-49e8-ab69-762492f0ca33', metadata={}, page_content='We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed on- line learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.'),\n",
       "  15.779577),\n",
       " (Document(id='0731a26a-d32a-4922-bd9a-544fabe24eb4', metadata={}, page_content='We study the effectiveness of neural sequence models for premise selection in automated theorem proving, a key bottleneck for progress in formalized mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied  theorem proving on a large scale.'),\n",
       "  16.483631),\n",
       " (Document(id='8a2538fc-09fe-4968-aeb6-0aa51825d49b', metadata={}, page_content='In this paper we explore two quantitative approaches to the modelling of counterfactual reasoning – a linear and a noisy-OR model – based on in- formation contained in conceptual dependency networks. Empirical data is acquired in a study and the ﬁt of the models compared to it. We con- clude by considering the appropriateness of non-parametric approaches to counterfactual reasoning, and examining the prospects for other para- metric approaches in the future.'),\n",
       "  16.650143)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['base_encoder'].similarity_search_with_score(query='Are there any pitfalls of using cross-validation for model selection?', k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all three results are irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:53:14.238450Z",
     "iopub.status.busy": "2025-05-22T12:53:14.237610Z",
     "iopub.status.idle": "2025-05-22T12:53:14.260412Z",
     "shell.execute_reply": "2025-05-22T12:53:14.259530Z",
     "shell.execute_reply.started": "2025-05-22T12:53:14.238417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='143b7a98-801b-4b09-bac7-c63650f6647a', metadata={}, page_content='Model selection is linked to model assessment, which is the problem of comparing different models, or model parameters, for a speciﬁc learning task. For supervised learning, the standard practical technique is cross- validation, which is not applicable for semi-supervised and unsupervised settings. In this paper, a new model assessment scheme is introduced which is based on a notion of stability. The stability measure yields an upper bound to cross-validation in the supervised case, but extends to semi-supervised and unsupervised problems. In the experimental part, the performance of the stability measure is studied for model order se- lection in comparison to standard techniques in this area.'),\n",
       "  84.082214),\n",
       " (Document(id='f0e00af4-1922-4a37-9d46-dbb661785714', metadata={}, page_content='We address the problem of non-parametric multiple model comparison: given $l$\\ncandidate models, decide whether each candidate is as good as the best one(s) or worse than it.  We propose two statistical tests,\\neach controlling a different notion of decision errors. The first test,\\nbuilding on the post selection inference framework, provably controls the\\nnumber of best models that are wrongly declared worse (false positive\\nrate). The second test is based on multiple correction, and controls the\\nproportion of the models declared worse but are in fact as good as the best\\n(false discovery rate). \\nWe prove that under appropriate conditions the first test can yield a higher true\\npositive rate than the second. Experimental results on toy and real (CelebA,\\nChicago Crime data) problems show that the two tests have high true positive\\nrates with well-controlled error rates. By contrast, the naive approach of\\nchoosing the model with the lowest score  without correction\\nleads to more false positives.'),\n",
       "  95.567825),\n",
       " (Document(id='9b50404e-2db4-4255-9eb8-7987947ac7ee', metadata={}, page_content=\"Two  theorems  and  a  lemma are  presented  about  the  use  of jackknife es(cid:173) timator and  the  cross-validation  method for  model selection.  Theorem  1  gives  the asymptotic form for  the jackknife estimator.  Combined with the  model selection  criterion,  this asymptotic form  can  be  used  to obtain the  fit  of a  model.  The model selection  criterion we  used  is the negative of the  average predictive likehood, the choice of which is  based on the idea of the  cross-validation  method.  Lemma 1  provides  a  formula  for  further  explo(cid:173) ration of the asymptotics of the model selection criterion.  Theorem 2 gives  an asymptotic form of the model selection criterion for  the regression  case,  when  the  parameters optimization criterion  has a  penalty term.  Theorem  2  also  proves  the  asymptotic equivalence  of Moody's  model selection  cri(cid:173) terion  (Moody,  1992)  and  the  cross-validation method,  when  the distance  measure  between  response  y  and  regression  function  takes  the  form  of a  squared  difference.\"),\n",
       "  97.70513)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores['finetuned_encoder'].similarity_search_with_score(query='Are there any pitfalls of using cross-validation for model selection?', k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pitfalls of cross-validation is highly specific query which the model cannot answer, most probably because no such information is present in the abstracts, however 1st and 3rd results are relevant to cross-validation, and 2nd result talks about model selection which is indirectly related to cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for quantitative evaluation of the retrieval results from the two models, we use LLM as a judge to provide us with relevancy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:57:56.335848Z",
     "iopub.status.busy": "2025-05-22T13:57:56.334991Z",
     "iopub.status.idle": "2025-05-22T13:57:56.349398Z",
     "shell.execute_reply": "2025-05-22T13:57:56.348563Z",
     "shell.execute_reply.started": "2025-05-22T13:57:56.335812Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_number(text: str) -> int:\n",
    "    # Try to find an isolated number between 0 and 10\n",
    "    match = re.search(r\"Score: (\\d+(\\.\\d+)?)?$\", text)\n",
    "    if match:\n",
    "        try:\n",
    "            int_match = match.group(1)\n",
    "        except:\n",
    "            int_match = 0\n",
    "        return int_match\n",
    "    return None\n",
    "\n",
    "class RAGEvaluator:\n",
    "\n",
    "    def __init__(self, llm, store):\n",
    "        self.llm = llm\n",
    "        self.store = store\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Tuple[str, float]]:\n",
    "        results = self.store.similarity_search_with_score(query, k=k)\n",
    "        return [(doc.page_content, score) for doc, score in results]\n",
    "\n",
    "    def score_with_llm(self, query: str, passage: str) -> Tuple[int, int]:\n",
    "        prompt_relevance = f\"\"\"You are an expert in text understanding. You have to determine how relevant is a document to a given question on a scale of integers 0-10. While giving the score, please think very carefully whether the given document is actually a good match to the question on various parameters of relevance. The input will be given to as a question and the document. You have to return your score from 0-10. Please do not output any additional tokens beyond the numerical score.\n",
    "        Question: {query}\n",
    "        Document: {passage}\n",
    "        Score: \"\"\"\n",
    "        prompt_binary = f\"\"\"You are an expert in text understanding. You have to determine how relevant is a document to a given question with a relevant match being 1 and irrelevant match being 0. While giving the score, please think very carefully whether the given document is actually a good match to the question on various parameters of relevance. The input will be given to as a question and the document. You have to return your score which can only be either 0 or 1. Please do not output any additional tokens beyond the numerical score.\n",
    "        Question: {query}\n",
    "        Document: {passage}\n",
    "        Score: \"\"\"\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        prompt_num = prompt_relevance.format(query=query, passage=passage)\n",
    "        prompt_bin = prompt_binary.format(query=query, passage=passage)\n",
    "    \n",
    "        rel_resp = str(local_llm(prompt_num)).strip()\n",
    "        bin_resp = str(local_llm(prompt_bin)).strip()\n",
    "    \n",
    "        try:\n",
    "            rel_resp_int = int(extract_number(text=rel_resp))\n",
    "        except:\n",
    "            rel_resp_int = 0\n",
    "        try:\n",
    "            bin_resp_int = int(extract_number(text=bin_resp))\n",
    "        except:\n",
    "            bin_resp_int = 0\n",
    "    \n",
    "        return rel_resp_int, bin_resp_int\n",
    "\n",
    "    def overlap_metrics(self, query: str, passage: str) -> Dict[str, int]:\n",
    "        def ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "            return [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "        q_tokens = query.split()\n",
    "        p_tokens = passage.split()\n",
    "        overlaps = {\n",
    "            'word_overlap': len(set(q_tokens) & set(p_tokens)),\n",
    "            '2gram_overlap': len(set(ngrams(q_tokens, 2)) & set(ngrams(p_tokens, 2))),\n",
    "            '3gram_overlap': len(set(ngrams(q_tokens, 3)) & set(ngrams(p_tokens, 3)))\n",
    "        }\n",
    "        return overlaps\n",
    "\n",
    "    def evaluate_retrieval(self, query: str, k: int = 3, use_llm: bool = True) -> Dict[str, Any]:\n",
    "        passages, retrieval_scores = zip(*self.retrieve(query, k))\n",
    "        rel_scores, bin_scores, overlaps = [], [], []\n",
    "        for p in passages:\n",
    "            rel, bin_ = self.score_with_llm(query, p) if use_llm else (0, 0)\n",
    "            rel_scores.append(rel)\n",
    "            bin_scores.append(bin_)\n",
    "            overlaps.append(self.overlap_metrics(query, p))\n",
    "\n",
    "        return {\n",
    "            'query': query,\n",
    "            'passages': passages,\n",
    "            'retrieval_scores': retrieval_scores,\n",
    "            'rel_scores': rel_scores,\n",
    "            'bin_scores': bin_scores,\n",
    "            'overlap': overlaps,\n",
    "            'precision@k': self.precision_at_k(bin_scores, k)\n",
    "        }\n",
    "\n",
    "    def precision_at_k(self, bin_scores: List[int], k: int) -> float:\n",
    "        return sum(bin_scores[:k]) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:57:59.290289Z",
     "iopub.status.busy": "2025-05-22T13:57:59.289722Z",
     "iopub.status.idle": "2025-05-22T13:57:59.294332Z",
     "shell.execute_reply": "2025-05-22T13:57:59.293486Z",
     "shell.execute_reply.started": "2025-05-22T13:57:59.290258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator_base = RAGEvaluator(local_llm, stores['base_encoder'])\n",
    "evaluator_finetuned = RAGEvaluator(local_llm, stores['finetuned_encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:44:26.977197Z",
     "iopub.status.busy": "2025-05-22T13:44:26.976467Z",
     "iopub.status.idle": "2025-05-22T13:44:26.998050Z",
     "shell.execute_reply": "2025-05-22T13:44:26.997275Z",
     "shell.execute_reply.started": "2025-05-22T13:44:26.977168Z"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1 = zip(*evaluator_base.retrieve(query = \"Which learning algorithms have been used to model brain activity in humans?\", k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:44:27.310614Z",
     "iopub.status.busy": "2025-05-22T13:44:27.310111Z",
     "iopub.status.idle": "2025-05-22T13:44:27.315256Z",
     "shell.execute_reply": "2025-05-22T13:44:27.314468Z",
     "shell.execute_reply.started": "2025-05-22T13:44:27.310589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.\",\n",
       " 'The  goal  in  this  work  has  been  to  identify  the  neuronal  elements  of the cortical column that are most likely to support  the learning  of nonlinear associative  maps.  We show that  a  particular style  of  network learning algorithm based on locally-tuned  receptive fields  maps  naturally  onto cortical  hardware,  and  gives  coherence  to  a  variety of features  of cortical anatomy,  physiology,  and  biophysics  whose  relations to learning remain poorly understood.',\n",
       " 'Neural network models have been criticized for their inability to make  use of compositional representations. In this paper, we describe a series  of psychological phenomena that demonstrate the role of structured  representations in cognition. These findings suggest that people  compare relational representations via a process of structural alignment.  This process will have to be captured by any model of cognition,  symbolic or subsymbolic.')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:44:27.753019Z",
     "iopub.status.busy": "2025-05-22T13:44:27.752736Z",
     "iopub.status.idle": "2025-05-22T13:44:27.758339Z",
     "shell.execute_reply": "2025-05-22T13:44:27.757525Z",
     "shell.execute_reply.started": "2025-05-22T13:44:27.752997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.56313, 17.2448, 17.45152)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:44:31.553270Z",
     "iopub.status.busy": "2025-05-22T13:44:31.552488Z",
     "iopub.status.idle": "2025-05-22T13:44:39.724034Z",
     "shell.execute_reply": "2025-05-22T13:44:39.723164Z",
     "shell.execute_reply.started": "2025-05-22T13:44:31.553238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Which learning algorithms have been used to model brain activity in humans?',\n",
       " 'passages': (\"Neuroscience studies of human decision-making abilities commonly involve\\nsubjects completing a decision-making task while BOLD signals are\\nrecorded using fMRI. Hypotheses are tested about which brain regions\\nmediate the effect of past experience, such as rewards, on future\\nactions. One standard approach to this is model-based fMRI data\\nanalysis, in which a model is fitted to the behavioral data, i.e., a\\nsubject's choices, and then the neural data are parsed to find brain\\nregions whose BOLD signals are related to the model's internal\\nsignals. However, the internal mechanics of such purely behavioral\\nmodels are not constrained by the neural data, and therefore might miss\\nor mischaracterize aspects of the brain. To address this limitation, we\\nintroduce a new method using recurrent neural network models that are\\nflexible enough to be jointly fitted to the behavioral and neural\\ndata. We trained a model so that its internal states were suitably\\nrelated to neural activity during the task, while at the same time its\\noutput predicted the next action a subject would execute. We then used\\nthe fitted model to create a novel visualization of the relationship\\nbetween the activity in brain regions at different times following a\\nreward and the choices the subject subsequently made. Finally, we\\nvalidated our method using a previously published dataset. We found that\\nthe model was able to recover the underlying neural substrates that were\\ndiscovered by explicit model engineering in the previous work, and also\\nderived new results regarding the temporal pattern of brain activity.\",\n",
       "  'Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied  by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies.',\n",
       "  'Areas of the brain involved in various forms of memory exhibit patterns           of neural activity quite unlike those in canonical computational models.           We show how to use well-founded Bayesian probabilistic autoassociative           recall to derive biologically reasonable neuronal dynamics in recurrently           coupled models, together with appropriate values for parameters such as           the membrane time constant and inhibition. We explicitly treat two cases.           One arises from a standard Hebbian learning rule, and involves activity           patterns that are coded by graded firing rates. The other arises from a           spike timing dependent learning rule, and involves patterns coded by the           phase of spike times relative to a coherent local field potential oscillation.           Our model offers a new and more complete understanding of how neural           dynamics may support autoassociation.'),\n",
       " 'retrieval_scores': (99.71602, 100.16124, 103.44913),\n",
       " 'rel_scores': [7, 0, 4],\n",
       " 'bin_scores': [1, 0, 0],\n",
       " 'overlap': [{'word_overlap': 6, '2gram_overlap': 1, '3gram_overlap': 0},\n",
       "  {'word_overlap': 4, '2gram_overlap': 1, '3gram_overlap': 0},\n",
       "  {'word_overlap': 6, '2gram_overlap': 0, '3gram_overlap': 0}],\n",
       " 'precision@k': 0.3333333333333333}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_finetuned.evaluate_retrieval(\"Which learning algorithms have been used to model brain activity in humans?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:45:19.526606Z",
     "iopub.status.busy": "2025-05-22T13:45:19.526162Z",
     "iopub.status.idle": "2025-05-22T13:45:26.944914Z",
     "shell.execute_reply": "2025-05-22T13:45:26.944040Z",
     "shell.execute_reply.started": "2025-05-22T13:45:19.526564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Which learning algorithms have been used to model brain activity in humans?',\n",
       " 'passages': (\"While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.\",\n",
       "  'The  goal  in  this  work  has  been  to  identify  the  neuronal  elements  of the cortical column that are most likely to support  the learning  of nonlinear associative  maps.  We show that  a  particular style  of  network learning algorithm based on locally-tuned  receptive fields  maps  naturally  onto cortical  hardware,  and  gives  coherence  to  a  variety of features  of cortical anatomy,  physiology,  and  biophysics  whose  relations to learning remain poorly understood.',\n",
       "  'Neural network models have been criticized for their inability to make  use of compositional representations. In this paper, we describe a series  of psychological phenomena that demonstrate the role of structured  representations in cognition. These findings suggest that people  compare relational representations via a process of structural alignment.  This process will have to be captured by any model of cognition,  symbolic or subsymbolic.'),\n",
       " 'retrieval_scores': (16.56313, 17.2448, 17.45152),\n",
       " 'rel_scores': [0, 3, 0],\n",
       " 'bin_scores': [0, 0, 0],\n",
       " 'overlap': [{'word_overlap': 2, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "  {'word_overlap': 4, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "  {'word_overlap': 5, '2gram_overlap': 1, '3gram_overlap': 0}],\n",
       " 'precision@k': 0.0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_base.evaluate_retrieval(\"Which learning algorithms have been used to model brain activity in humans?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### send a list of queries to both the encoders to evaluate which one has better retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:48:52.674251Z",
     "iopub.status.busy": "2025-05-22T13:48:52.673642Z",
     "iopub.status.idle": "2025-05-22T13:48:52.680957Z",
     "shell.execute_reply": "2025-05-22T13:48:52.680077Z",
     "shell.execute_reply.started": "2025-05-22T13:48:52.674217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a list of questions for which you want literature review\n",
    "\n",
    "query_list = [\n",
    "    \"What are some breakthroughs in theoretical understanding of neural networks?\",\n",
    "    \"Explain how random forest has been applied for medical use cases.\",\n",
    "    \"Compare the performance of random forests and XGBoost, and explain which algorithm performs better in which case.\",\n",
    "    \"Is there practical research where Uniform Convergence property has been used to compare different learning algorithms?\",\n",
    "    \"Datasets where the framework of PAC learning is demonstrated to work.\",\n",
    "    \"Are there any pitfalls of using cross-validation for model selection?\",\n",
    "    \"Theoretical justifications for effects of different hyper-parameters on the performance of random forest?\",\n",
    "    \"How is central limit theorem useful for analysis of model performance and convergence of learning algorithms?\",\n",
    "    \"Use of Hilbert space in machine learning.\",\n",
    "    \"How does high-dimensional data affect different learning algorithms.\",\n",
    "    \"Which learning algorithms have been used to model brain activity in humans?\",\n",
    "    \"Algorithms used for ranking in recommender systems.\",\n",
    "    \"Why is normalization needed for co-occurrence matrices in recommendation system?\",\n",
    "    \"Practical datasets which compare the performance of PCA and kernel PCA.\",\n",
    "    \"Is regularization really needed in neural networks?\",\n",
    "    \"Any techniques which have the effect of implicit regularization in neural networks?\",\n",
    "    \"What does batch normalization really do mathematically in neural networks?\",\n",
    "    \"Comparison of fraud detection methods in banking.\",\n",
    "    \"Mathematical understanding of weight initialization methods in neural networks.\",\n",
    "    \"Practical cases where less powerful models achieve better performance than more powerful models.\"\n",
    "]\n",
    "\n",
    "len(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:53:02.103379Z",
     "iopub.status.busy": "2025-05-22T13:53:02.102985Z",
     "iopub.status.idle": "2025-05-22T13:53:02.107852Z",
     "shell.execute_reply": "2025-05-22T13:53:02.106831Z",
     "shell.execute_reply.started": "2025-05-22T13:53:02.103338Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:58:05.134976Z",
     "iopub.status.busy": "2025-05-22T13:58:05.134011Z",
     "iopub.status.idle": "2025-05-22T14:00:32.044532Z",
     "shell.execute_reply": "2025-05-22T14:00:32.043611Z",
     "shell.execute_reply.started": "2025-05-22T13:58:05.134932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:26<00:00,  7.35s/it]\n"
     ]
    }
   ],
   "source": [
    "base_encoder_evaluations = []\n",
    "\n",
    "for query in tqdm(query_list):\n",
    "    try:\n",
    "        base_encoder_evaluations.append(evaluator_base.evaluate_retrieval(query))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:00:33.778098Z",
     "iopub.status.busy": "2025-05-22T14:00:33.777760Z",
     "iopub.status.idle": "2025-05-22T14:00:33.785085Z",
     "shell.execute_reply": "2025-05-22T14:00:33.784215Z",
     "shell.execute_reply.started": "2025-05-22T14:00:33.778071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'What are some breakthroughs in theoretical understanding of neural networks?',\n",
       "  'passages': ('A short account is  given of various  investigations of neural  network  properties,  beginning  with  the  classic  work of McCulloch  & Pitts.  Early work on neurodynamics and statistical mechanics, analogies with  magnetic materials, fault tolerance via parallel distributed processing,  memory, learning,  and pattern recognition,  is described.',\n",
       "   'With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory.  We then investigate how well the measures explain different observed phenomena.',\n",
       "   'In this paper we investigate an average-case model of concept learning, and  give results that place the popular statistical physics and VC dimension  theories of learning curve behavior in a common framework.'),\n",
       "  'retrieval_scores': (20.5022, 20.520811, 20.98244),\n",
       "  'rel_scores': [7, 7, 1],\n",
       "  'bin_scores': [1, 1, 0],\n",
       "  'overlap': [{'word_overlap': 2, '2gram_overlap': 1, '3gram_overlap': 0},\n",
       "   {'word_overlap': 3, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "   {'word_overlap': 2, '2gram_overlap': 0, '3gram_overlap': 0}],\n",
       "  'precision@k': 0.6666666666666666},\n",
       " {'query': 'Explain how random forest has been applied for medical use cases.',\n",
       "  'passages': ('We consider off-policy evaluation (OPE) in continuous treatment settings, such as personalized dose-finding. In OPE, one aims to estimate the mean outcome under a new treatment decision rule using historical data generated by a different decision rule. Most existing works on OPE focus on discrete treatment settings. To handle continuous treatments, we develop a novel estimation method for OPE using deep jump learning. The key ingredient of our method lies in adaptively discretizing the treatment space using deep discretization, by leveraging deep learning and multi-scale change point detection. This allows us to apply existing OPE methods in discrete treatments to handle continuous treatments. Our method is further justified by theoretical results, simulations, and a real application to Warfarin Dosing.',\n",
       "   'The power of sampling methods in Bayesian reconstruction of noisy  signals is well known.  The extension of sampling to temporal prob(cid:173) lems  is  discussed.  Efficacy  of sampling over  time is  demonstrated  with visual tracking.',\n",
       "   'Diagnosis of faults in complex, real-time control systems is a  complicated task that has resisted solution by traditional methods. We  have shown that neural networks can be successfully employed to  diagnose faults in digitally controlled powertrain systems. This paper  discusses the means we use to develop the appropriate databases for  training and testing in order to select the optimum network architectures  and to provide reasonable estimates of the classification accuracy of  these networks on new samples of data. Recent work applying neural  nets to adaptive control of an active suspension system is presented.'),\n",
       "  'retrieval_scores': (17.251038, 17.46783, 17.62994),\n",
       "  'rel_scores': [0, 0, 0],\n",
       "  'bin_scores': [0, 0, 0],\n",
       "  'overlap': [{'word_overlap': 1, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "   {'word_overlap': 0, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "   {'word_overlap': 3, '2gram_overlap': 0, '3gram_overlap': 0}],\n",
       "  'precision@k': 0.0},\n",
       " {'query': 'Compare the performance of random forests and XGBoost, and explain which algorithm performs better in which case.',\n",
       "  'passages': ('We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an LP relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation.',\n",
       "   \"We suggest a general oracle-based framework that captures parallel\\n  stochastic optimization in different parallelization settings\\n  described by a dependency graph, and derive generic lower bounds \\n  in terms of this graph.  We then use the framework and derive lower\\n  bounds to study several specific parallel optimization settings,\\n  including delayed updates and parallel processing with intermittent\\n  communication.  We highlight gaps between lower and upper bounds on\\n  the oracle complexity, and cases where the ``natural'' algorithms\\n  are not known to be optimal.\",\n",
       "   'We introduce a differentiable clustering method based on stochastic perturbations of minimum-weight spanning forests. This allows us to include clustering in end-to-end trainable pipelines, with efficient gradients. We show that our method performs well even in difficult settings, such as data sets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several data sets for supervised and semi-supervised tasks.'),\n",
       "  'retrieval_scores': (11.276674, 11.77939, 11.790126),\n",
       "  'rel_scores': [0, 0, 0],\n",
       "  'bin_scores': [0, 0, 0],\n",
       "  'overlap': [{'word_overlap': 5, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "   {'word_overlap': 4, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "   {'word_overlap': 5, '2gram_overlap': 0, '3gram_overlap': 0}],\n",
       "  'precision@k': 0.0}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_encoder_evaluations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:01:44.362457Z",
     "iopub.status.busy": "2025-05-22T14:01:44.361750Z",
     "iopub.status.idle": "2025-05-22T14:03:54.345022Z",
     "shell.execute_reply": "2025-05-22T14:03:54.344200Z",
     "shell.execute_reply.started": "2025-05-22T14:01:44.362425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:09<00:00,  6.50s/it]\n"
     ]
    }
   ],
   "source": [
    "finetuned_encoder_evaluations = []\n",
    "\n",
    "for query in tqdm(query_list):\n",
    "    try:\n",
    "        finetuned_encoder_evaluations.append(evaluator_finetuned.evaluate_retrieval(query))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:04:08.349923Z",
     "iopub.status.busy": "2025-05-22T14:04:08.349564Z",
     "iopub.status.idle": "2025-05-22T14:04:08.357337Z",
     "shell.execute_reply": "2025-05-22T14:04:08.356400Z",
     "shell.execute_reply.started": "2025-05-22T14:04:08.349894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'What are some breakthroughs in theoretical understanding of neural networks?',\n",
       "  'passages': ('A short account is  given of various  investigations of neural  network  properties,  beginning  with  the  classic  work of McCulloch  & Pitts.  Early work on neurodynamics and statistical mechanics, analogies with  magnetic materials, fault tolerance via parallel distributed processing,  memory, learning,  and pattern recognition,  is described.',\n",
       "   'The success of Deep Learning and its potential use in many safety-critical\\n  applications has motivated research on formal verification of Neural Network\\n  (NN) models. Despite the reputation of learned NN models to behave as black\\n  boxes and the theoretical hardness of proving their properties, researchers\\n  have been successful in verifying some classes of models by exploiting their\\n  piecewise linear structure and taking insights from formal methods such as\\n  Satisifiability Modulo Theory. These methods are however still far from\\n  scaling to realistic neural networks. To facilitate progress on this crucial\\n  area, we make two key contributions. First, we present a unified framework\\n  that encompasses previous methods. This analysis results in the identification\\n  of new methods that combine the strengths of multiple existing approaches,\\n  accomplishing a speedup of two orders of magnitude compared to the previous\\n  state of the art. Second, we propose a new data set of benchmarks which\\n  includes a collection of previously released testcases. We use the benchmark\\n  to provide the first experimental comparison of existing algorithms and\\n  identify the factors impacting the hardness of verification problems.',\n",
       "   'Although color TV is an established technology, there are a number of  longstanding problems for which neural networks may be suited. Impulse  noise is such a problem, and a modular neural network approach is pre(cid:173) sented in this paper. The training and analysis was done on conventional  computers, while real-time simulations were performed on a massively par(cid:173) allel computer called the Princeton Engine. The network approach was  compared to a conventional alternative, a median filter. Real-time simula(cid:173) tions and quantitative analysis demonstrated the technical superiority of  the neural system. Ongoing work is investigating the complexity and cost  of implementing this system in hardware. 1 THE POTENTIAL FOR NEURAL NETWORKS IN CONSUMER ELECTRONICS Neural networks are most often considered for application in emerging new tech(cid:173) nologies, such as speech recognition, machine vision, and robotics. The fundamental  ideas behind these technologies are still being developed, and it will be some time  before products containing neural networks are manufactured. As a result, research  in these areas will not drive the development of inexpensive neural network hard(cid:173) ware which could serve as a catalyst for the field of neural networks in general. In contrast, neural networks are rarely considered for application in mature tech(cid:173) nologies, such as consumer electronics. These technologies are based on established  principles of information processing and communication, and they are used in mil(cid:173) lions of products per year. The embedding of neural networks within such mass-'),\n",
       "  'retrieval_scores': (83.8517, 92.33951, 92.821884),\n",
       "  'rel_scores': [7, 0, 0],\n",
       "  'bin_scores': [1, 0, 0],\n",
       "  'overlap': [{'word_overlap': 2, '2gram_overlap': 1, '3gram_overlap': 0},\n",
       "   {'word_overlap': 6, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "   {'word_overlap': 5, '2gram_overlap': 1, '3gram_overlap': 0}],\n",
       "  'precision@k': 0.3333333333333333},\n",
       " {'query': 'Datasets where the framework of PAC learning is demonstrated to work.',\n",
       "  'passages': ('We focus on a stochastic learning model where the learner observes a finite set of training examples and the output of the learning process is a data-dependent distribution over a space of hypotheses. The learned data-dependent distribution is then used to make randomized predictions, and the high-level theme addressed here is guaranteeing the quality of predictions on examples that were not seen during training, i.e. generalization. In this setting the unknown quantity of interest is the expected risk of the data-dependent randomized predictor, for which upper bounds can be derived via a PAC-Bayes analysis, leading to PAC-Bayes bounds.Specifically, we present a basic PAC-Bayes inequality for stochastic kernels, from which one may derive extensions of various known PAC-Bayes bounds as well as novel bounds.  We clarify the role of the requirements of fixed ‘data-free’ priors, bounded losses, and i.i.d. data. We highlight that those requirements were used to upper-bound an exponential moment term, while the basic PAC-Bayes theorem remains valid without those restrictions. We present three bounds that illustrate the use of data-dependent priors, including one for the unbounded square loss.',\n",
       "   'In this work we aim at extending theoretical foundations of lifelong learning. Previous work analyzing this scenario is based on the assumption that the tasks are sampled i.i.d. from a task environment or limited to strongly constrained data distributions. Instead we study two scenarios when lifelong learning is possible, even though the observed tasks do not form an i.i.d. sample: first, when they are sampled from the same environment, but possibly with dependencies, and second, when the task environment is allowed to change over time. In the first case we prove a PAC-Bayesian theorem, which can be seen as a direct generalization of the analogous previous result for the i.i.d. case. For the second scenario we propose to learn an inductive bias in form of a transfer procedure. We present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm.',\n",
       "   'Despite recent advances in its theoretical understanding, there still remains a significant gap in the ability of existing PAC-Bayesian theories on meta-learning to explain performance improvements in the few-shot learning setting, where the number of training examples in the target tasks is severely limited. This gap originates from an assumption in the existing theories which supposes that the number of training examples in the observed tasks and the number of training examples in the target tasks follow the same distribution, an assumption that rarely holds in practice. By relaxing this assumption, we develop two PAC-Bayesian bounds tailored for the few-shot learning setting and show that two existing meta-learning algorithms (MAML and Reptile) can be derived from our bounds, thereby bridging the gap between practice and PAC-Bayesian theories. Furthermore, we derive a new computationally-efficient PACMAML algorithm, and show it outperforms existing meta-learning algorithms on several few-shot benchmark datasets.'),\n",
       "  'retrieval_scores': (79.30565, 82.331764, 84.692245),\n",
       "  'rel_scores': [3, 4, 2],\n",
       "  'bin_scores': [0, 0, 0],\n",
       "  'overlap': [{'word_overlap': 6, '2gram_overlap': 1, '3gram_overlap': 0},\n",
       "   {'word_overlap': 5, '2gram_overlap': 1, '3gram_overlap': 0},\n",
       "   {'word_overlap': 6, '2gram_overlap': 1, '3gram_overlap': 0}],\n",
       "  'precision@k': 0.0},\n",
       " {'query': 'Are there any pitfalls of using cross-validation for model selection?',\n",
       "  'passages': ('Model selection is linked to model assessment, which is the problem of comparing different models, or model parameters, for a speciﬁc learning task. For supervised learning, the standard practical technique is cross- validation, which is not applicable for semi-supervised and unsupervised settings. In this paper, a new model assessment scheme is introduced which is based on a notion of stability. The stability measure yields an upper bound to cross-validation in the supervised case, but extends to semi-supervised and unsupervised problems. In the experimental part, the performance of the stability measure is studied for model order se- lection in comparison to standard techniques in this area.',\n",
       "   'We address the problem of non-parametric multiple model comparison: given $l$\\ncandidate models, decide whether each candidate is as good as the best one(s) or worse than it.  We propose two statistical tests,\\neach controlling a different notion of decision errors. The first test,\\nbuilding on the post selection inference framework, provably controls the\\nnumber of best models that are wrongly declared worse (false positive\\nrate). The second test is based on multiple correction, and controls the\\nproportion of the models declared worse but are in fact as good as the best\\n(false discovery rate). \\nWe prove that under appropriate conditions the first test can yield a higher true\\npositive rate than the second. Experimental results on toy and real (CelebA,\\nChicago Crime data) problems show that the two tests have high true positive\\nrates with well-controlled error rates. By contrast, the naive approach of\\nchoosing the model with the lowest score  without correction\\nleads to more false positives.',\n",
       "   \"Two  theorems  and  a  lemma are  presented  about  the  use  of jackknife es(cid:173) timator and  the  cross-validation  method for  model selection.  Theorem  1  gives  the asymptotic form for  the jackknife estimator.  Combined with the  model selection  criterion,  this asymptotic form  can  be  used  to obtain the  fit  of a  model.  The model selection  criterion we  used  is the negative of the  average predictive likehood, the choice of which is  based on the idea of the  cross-validation  method.  Lemma 1  provides  a  formula  for  further  explo(cid:173) ration of the asymptotics of the model selection criterion.  Theorem 2 gives  an asymptotic form of the model selection criterion for  the regression  case,  when  the  parameters optimization criterion  has a  penalty term.  Theorem  2  also  proves  the  asymptotic equivalence  of Moody's  model selection  cri(cid:173) terion  (Moody,  1992)  and  the  cross-validation method,  when  the distance  measure  between  response  y  and  regression  function  takes  the  form  of a  squared  difference.\"),\n",
       "  'retrieval_scores': (84.082214, 95.567825, 97.70513),\n",
       "  'rel_scores': [7, 0, 8],\n",
       "  'bin_scores': [1, 0, 1],\n",
       "  'overlap': [{'word_overlap': 4, '2gram_overlap': 1, '3gram_overlap': 0},\n",
       "   {'word_overlap': 2, '2gram_overlap': 0, '3gram_overlap': 0},\n",
       "   {'word_overlap': 4, '2gram_overlap': 1, '3gram_overlap': 0}],\n",
       "  'precision@k': 0.6666666666666666}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_encoder_evaluations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:04:32.904000Z",
     "iopub.status.busy": "2025-05-22T14:04:32.903626Z",
     "iopub.status.idle": "2025-05-22T14:04:32.908782Z",
     "shell.execute_reply": "2025-05-22T14:04:32.907916Z",
     "shell.execute_reply.started": "2025-05-22T14:04:32.903969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(base_encoder_evaluations))\n",
    "print(len(finetuned_encoder_evaluations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8 cases where some error was there, fix later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:06:50.640610Z",
     "iopub.status.busy": "2025-05-22T14:06:50.640237Z",
     "iopub.status.idle": "2025-05-22T14:06:50.646680Z",
     "shell.execute_reply": "2025-05-22T14:06:50.645895Z",
     "shell.execute_reply.started": "2025-05-22T14:06:50.640577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666666666666666"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_relevance_base_encoder = sum([sum(ev['rel_scores'])/3 for ev in base_encoder_evaluations]) / len(base_encoder_evaluations)\n",
    "avg_relevance_base_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:07:22.255522Z",
     "iopub.status.busy": "2025-05-22T14:07:22.254960Z",
     "iopub.status.idle": "2025-05-22T14:07:22.261359Z",
     "shell.execute_reply": "2025-05-22T14:07:22.260420Z",
     "shell.execute_reply.started": "2025-05-22T14:07:22.255495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2499999999999996"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_relevance_finetuned_encoder = sum([sum(ev['rel_scores'])/3 for ev in finetuned_encoder_evaluations]) / len(finetuned_encoder_evaluations)\n",
    "avg_relevance_finetuned_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average retrieval relevance scores of the finetuned encoder is quite better than the base encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:09:02.459901Z",
     "iopub.status.busy": "2025-05-22T14:09:02.459240Z",
     "iopub.status.idle": "2025-05-22T14:09:02.465717Z",
     "shell.execute_reply": "2025-05-22T14:09:02.464741Z",
     "shell.execute_reply.started": "2025-05-22T14:09:02.459846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_base_encoder = sum([sum(ev['bin_scores'])/3 for ev in base_encoder_evaluations]) / len(base_encoder_evaluations)\n",
    "avg_precision_base_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:09:03.932856Z",
     "iopub.status.busy": "2025-05-22T14:09:03.932279Z",
     "iopub.status.idle": "2025-05-22T14:09:03.938516Z",
     "shell.execute_reply": "2025-05-22T14:09:03.937581Z",
     "shell.execute_reply.started": "2025-05-22T14:09:03.932830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3055555555555555"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_finetuned_encoder = sum([sum(ev['bin_scores'])/3 for ev in finetuned_encoder_evaluations]) / len(finetuned_encoder_evaluations)\n",
    "avg_precision_finetuned_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average precision scores of the finetuned encoder is also quite better than the base encoder, therefore we can use the finetuned encoder for the retrieval part of our RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generation using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:14:00.378993Z",
     "iopub.status.busy": "2025-05-22T14:14:00.378612Z",
     "iopub.status.idle": "2025-05-22T14:14:00.385697Z",
     "shell.execute_reply": "2025-05-22T14:14:00.384809Z",
     "shell.execute_reply.started": "2025-05-22T14:14:00.378963Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generation_pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000\n",
    ")\n",
    "generation_llm = HuggingFacePipeline(pipeline=generation_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:28:59.656651Z",
     "iopub.status.busy": "2025-05-22T14:28:59.655833Z",
     "iopub.status.idle": "2025-05-22T14:28:59.678300Z",
     "shell.execute_reply": "2025-05-22T14:28:59.677468Z",
     "shell.execute_reply.started": "2025-05-22T14:28:59.656620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query: What are some breakthroughs in theoretical understanding of neural networks? \n",
      " Prompt: Use the context which is provided to answer the given query.  The context is a set of technical\n",
      "papers which are not suitable for clear understanding for a common person. Summarize your answer and in such a way that an undergraduate student\n",
      "can also understand it. I also need the summary in a structured way, like someone would write a literature review. \n",
      " Context: Document 1: Neuroscience studies of human decision-making abilities commonly involve\n",
      "subjects completing a decision-making task while BOLD signals are\n",
      "recorded using fMRI. Hypotheses are tested about which brain regions\n",
      "mediate the effect of past experience, such as rewards, on future\n",
      "actions. One standard approach to this is model-based fMRI data\n",
      "analysis, in which a model is fitted to the behavioral data, i.e., a\n",
      "subject's choices, and then the neural data are parsed to find brain\n",
      "regions whose BOLD signals are related to the model's internal\n",
      "signals. However, the internal mechanics of such purely behavioral\n",
      "models are not constrained by the neural data, and therefore might miss\n",
      "or mischaracterize aspects of the brain. To address this limitation, we\n",
      "introduce a new method using recurrent neural network models that are\n",
      "flexible enough to be jointly fitted to the behavioral and neural\n",
      "data. We trained a model so that its internal states were suitably\n",
      "related to neural activity during the task, while at the same time its\n",
      "output predicted the next action a subject would execute. We then used\n",
      "the fitted model to create a novel visualization of the relationship\n",
      "between the activity in brain regions at different times following a\n",
      "reward and the choices the subject subsequently made. Finally, we\n",
      "validated our method using a previously published dataset. We found that\n",
      "the model was able to recover the underlying neural substrates that were\n",
      "discovered by explicit model engineering in the previous work, and also\n",
      "derived new results regarding the temporal pattern of brain activity. \n",
      "Document 2: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied  by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. \n",
      "Document 3: Areas of the brain involved in various forms of memory exhibit patterns           of neural activity quite unlike those in canonical computational models.           We show how to use well-founded Bayesian probabilistic autoassociative           recall to derive biologically reasonable neuronal dynamics in recurrently           coupled models, together with appropriate values for parameters such as           the membrane time constant and inhibition. We explicitly treat two cases.           One arises from a standard Hebbian learning rule, and involves activity           patterns that are coded by graded firing rates. The other arises from a           spike timing dependent learning rule, and involves patterns coded by the           phase of spike times relative to a coherent local field potential oscillation.           Our model offers a new and more complete understanding of how neural           dynamics may support autoassociation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a response based on the combined top 3 abstracts\n",
    "query = query_list[0]\n",
    "retrieved_docs, retrieved_scores = zip(*evaluator_finetuned.retrieve(query = \"Which learning algorithms have been used to model brain activity in humans?\", k=3))\n",
    "context = f\"\"\n",
    "for idx, d in enumerate(retrieved_docs):\n",
    "    context += f\"Document {idx+1}: {d} \\n\"\n",
    "prompt = f\"\"\" Query: {query} \\n Prompt: Use the context which is provided to answer the given query.  The context is a set of technical\n",
    "papers which are not suitable for clear understanding for a common person. Summarize your answer and in such a way that an undergraduate student\n",
    "can also understand it. I also need the summary in a structured way, like someone would write a literature review. \\n Context: {context}\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T14:29:05.045743Z",
     "iopub.status.busy": "2025-05-22T14:29:05.044927Z",
     "iopub.status.idle": "2025-05-22T14:30:58.323825Z",
     "shell.execute_reply": "2025-05-22T14:30:58.322830Z",
     "shell.execute_reply.started": "2025-05-22T14:29:05.045709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:  Query: What are some breakthroughs in theoretical understanding of neural networks? \n",
      " Prompt: Use the context which is provided to answer the given query.  The context is a set of technical\n",
      "papers which are not suitable for clear understanding for a common person. Summarize your answer and in such a way that an undergraduate student\n",
      "can also understand it. I also need the summary in a structured way, like someone would write a literature review. \n",
      " Context: Document 1: Neuroscience studies of human decision-making abilities commonly involve\n",
      "subjects completing a decision-making task while BOLD signals are\n",
      "recorded using fMRI. Hypotheses are tested about which brain regions\n",
      "mediate the effect of past experience, such as rewards, on future\n",
      "actions. One standard approach to this is model-based fMRI data\n",
      "analysis, in which a model is fitted to the behavioral data, i.e., a\n",
      "subject's choices, and then the neural data are parsed to find brain\n",
      "regions whose BOLD signals are related to the model's internal\n",
      "signals. However, the internal mechanics of such purely behavioral\n",
      "models are not constrained by the neural data, and therefore might miss\n",
      "or mischaracterize aspects of the brain. To address this limitation, we\n",
      "introduce a new method using recurrent neural network models that are\n",
      "flexible enough to be jointly fitted to the behavioral and neural\n",
      "data. We trained a model so that its internal states were suitably\n",
      "related to neural activity during the task, while at the same time its\n",
      "output predicted the next action a subject would execute. We then used\n",
      "the fitted model to create a novel visualization of the relationship\n",
      "between the activity in brain regions at different times following a\n",
      "reward and the choices the subject subsequently made. Finally, we\n",
      "validated our method using a previously published dataset. We found that\n",
      "the model was able to recover the underlying neural substrates that were\n",
      "discovered by explicit model engineering in the previous work, and also\n",
      "derived new results regarding the temporal pattern of brain activity. \n",
      "Document 2: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied  by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. \n",
      "Document 3: Areas of the brain involved in various forms of memory exhibit patterns           of neural activity quite unlike those in canonical computational models.           We show how to use well-founded Bayesian probabilistic autoassociative           recall to derive biologically reasonable neuronal dynamics in recurrently           coupled models, together with appropriate values for parameters such as           the membrane time constant and inhibition. We explicitly treat two cases.           One arises from a standard Hebbian learning rule, and involves activity           patterns that are coded by graded firing rates. The other arises from a           spike timing dependent learning rule, and involves patterns coded by the           phase of spike times relative to a coherent local field potential oscillation.           Our model offers a new and more complete understanding of how neural           dynamics may support autoassociation. \n",
      " \n",
      "\n",
      "## Breakthroughs in Theoretical Understanding of Neural Networks: A Literature Review\n",
      "\n",
      "This review summarizes recent breakthroughs in our theoretical understanding of neural networks, drawing insights from three key research papers.\n",
      "\n",
      "**1. Bridging the Gap Between Behavior and Neural Data:**\n",
      "\n",
      "Document 1 highlights a significant challenge in neuroscience: reconciling behavioral models with neural data. Traditional model-based fMRI analysis often relies on purely behavioral models, which lack constraints from neural activity and may miss crucial aspects of brain function.\n",
      "\n",
      "The authors propose a novel approach using recurrent neural networks (RNNs) that are jointly fitted to both behavioral and neural data. This allows the model to learn a representation of the brain's internal states that is directly related to neural activity while also accurately predicting behavior. This method offers a more comprehensive understanding of how brain activity contributes to decision-making.\n",
      "\n",
      "**2. Towards a Bidirectional Understanding of Brain Function:**\n",
      "\n",
      "Document 2 addresses the limitations of traditional correlational studies in neuroscience, which often establish a unidirectional link between brain activity and behavior.\n",
      "\n",
      "The authors propose a novel methodology that leverages a large corpus of imaging studies and a predictive engine to establish a bidirectional link between brain activity and cognitive function. By using a cognitive ontology to label tasks and modeling rare paradigms, they demonstrate the ability to predict the cognitive content of completely new brain images. This approach paves the way for a more comprehensive understanding of how different brain regions contribute to various cognitive functions.\n",
      "\n",
      "**3. Biologically Plausible Neural Dynamics:**\n",
      "\n",
      "Document 3 focuses on the discrepancies between observed neural activity patterns in memory-related brain areas and canonical computational models.\n",
      "\n",
      "The authors propose a new framework based on Bayesian probabilistic autoassociative recall, which allows them to derive biologically plausible neuronal dynamics in recurrently coupled models. They demonstrate how different learning rules, such as Hebbian and spike timing-dependent plasticity, can lead to distinct coding schemes for memory patterns, involving either graded firing rates or phase-locked spike times. This work provides a more complete understanding of how neural dynamics may support autoassociation and memory formation.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "These breakthroughs demonstrate significant progress in our theoretical understanding of neural networks. By integrating behavioral and neural data, leveraging large-scale datasets, and incorporating biologically plausible mechanisms, researchers are gaining deeper insights into the complex workings of the brain. These advancements hold promise for developing more accurate models of brain function and ultimately leading to new treatments for neurological disorders.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = generation_llm(prompt)\n",
    "print(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5570351,
     "sourceId": 9212146,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7486356,
     "sourceId": 11908570,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72256,
     "sourceId": 104453,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72254,
     "sourceId": 104623,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
